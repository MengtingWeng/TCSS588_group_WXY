
#Format of parameter "data", samples in rows and attributes in columns 
#Model: the model function
#evalMethod: the evaluation Method function

library(ROCR)
library(pROC)
library(ada)
library(e1071)
#================================Usage================================#
#use set.seed(a.seed) from R command line as you wish for your debugging replicable purpose

#input 
 #data:dataframe contains predictors and target variable
 #round: NO of rounds we want to repeat cross-validation
 #Model:the function name of the model the parameter of the model should be the same as sample function nb
 #K: no of folds for crossValidation
 #M:No of bagging iteration (it can be set to 20)
 #X: no of sample from each of y=0 and y=1 (it can be set to 50 or 40)

#output is data frame contains:
  #groundtruth-class, round and fold for each row as well as predition in each bag

kfoldCV <- function(data,round,Model,k,M,X)
{ 
  
  #Creat prediction dataframe to save result of each round
  predictionRound<-NULL

  for(j in 1:round)
  { 
    #Randomly shuffle the data
    data<-data[sample(nrow(data)),]
    
    #Create k roughly equally size folds
    folds <- cut(seq(1,nrow(data)),breaks=k,labels=FALSE)
    
    #Creat prediction dataframe to save result of each round
    predictionFold<-NULL
  
    #Perform k fold cross validation
    for(i in 1:k){
      cat("\n\n round ", j ," fold: ", i )
      #Segement data by fold  
      testIndexes <- which(folds==i,arr.ind=TRUE)
      testData <- data[testIndexes, 1:ncol(data)]
      trainData <- data[-testIndexes, 1:ncol(data)]
      testClass<-data[testIndexes, c(1,1,1)]
      names(testClass)[2]<-"round"
      testClass$round<-j
      names(testClass)[3]<-"fold"
      testClass$fold<-i
      #Train model on imBalance Data
      testClass$p<-Model(trainData,testData)
      
      #perform bagging process
      for(m in 1:M)
      {
        #UnderSampling of both class
        dataP = trainData[which(trainData$trainingClass == 1),]
        dataN = trainData[which(trainData$trainingClass == 0),]        
        sampleDataP = dataP[sample(x=1:nrow(dataP),size=X,replace=TRUE),]
        sampleDataN = dataN[sample(x=1:nrow(dataN),size=X,replace=TRUE),]
        undersampleTrain <- rbind(sampleDataP,sampleDataN)
        name<-gsub(" ","",paste("p",m))
        #Train model on Balance Data
        testClass[,name]<-Model( undersampleTrain,testData)
        
      }
      
      predictionFold <- rbind(predictionFold,testClass)
      
    }
    predictionRound <- rbind(predictionRound,predictionFold)
  }
  return(predictionRound)
}
##==================================================evalMethod
#evalMethod: the evaluation Method function to calculate the bac and AUC
#input
 #pred: dataframe generated by kfoldCV function
 #bagging: no of bagging that set in kfoldCV 
 #th:the threshold for classlabel(usually set to 0.5)

# TCSS 588: you don't need to evaluate your method using BAC and AUC
# just compute the number of errors is sufficient.
# for TCSS 588 hw3, change functions "evalMethod", "bac", "auc"

evalMethod<-function(pred,bagging,th)
{
  
  round=max(pred$round)
  fold=max(pred$fold)
  #columns number respect to boosting
  col=4+bagging
  #average 
  pred$avg<-rowMeans(pred[,5:col])
  #calculate majority 
  pred$countth<-rowSums(pred[,5:col]>th)
  pred$majority<-ifelse(pred$countth>(bagging/2),1,0)
  
  #Variables for temprary result
  BAC <- NULL
  AUC<-NULL
  AUC.avg<-NULL
  BAC.avg<-NULL
  BAC.majority<-NULL
  
  
  for(j in 1:round)
  {
    #for averaging the BAC
    aBAC <- NULL
    aAUC <- NULL
    aBAC.avg<-NULL
    aAUC.avg<-NULL
    aBAC.majority<-NULL
    
    for(i in 1:fold)
    {
      cat("\n round ", j ," fold: ", i )
      
      evaluation.data<- subset(pred, pred$round==j & pred$fold==i)   
      #scoring on testing fold
      #AUC without bagging
      foldAUC <- auc(evaluation.data$trainingClass,evaluation.data$p)
      aAUC <- c(aAUC,foldAUC)
     
      #BAC without bagging
      foldBAC <- bac(evaluation.data$trainingClass,evaluation.data$p,th)
      aBAC <- c(aBAC,foldBAC)
      
      #BAC with bagging avg
      foldBAC.avg <- bac(evaluation.data$trainingClass,evaluation.data$avg,th)
      aBAC.avg <- c(aBAC.avg,foldBAC.avg)
      
      #AUC with bagging-avg
      foldAUC.avg <- auc(evaluation.data$trainingClass,evaluation.data$avg)
      aAUC.avg <- c(aAUC.avg,foldAUC.avg)
      
      #BAC with bagging- majority
      foldBAC.majority <- bac(evaluation.data$trainingClass,evaluation.data$majority,th)
      aBAC.majority <- c(aBAC.majority,foldBAC.majority)
      
      #show statistics in output
      cat("\nfold " , i , "round " , j ,"AUC: ", foldAUC)
      cat("\nfold " , i , "round " , j ,"bac: ", foldBAC)
      cat("\nfold " , i , "round " , j ,"bac of avg: ", foldBAC.avg)
      cat("\nfold " , i , "round " , j ,"AUC of avg: ", foldAUC.avg)
      cat("\nfold " , i , "round " , j ,"bac of majority: ", foldBAC.majority,"\n\n")
    }
    
    #Scoring on whole round 
    AUC<-c(AUC,mean(aAUC))
    AUC.avg<-c(AUC.avg,mean(aAUC.avg))
    BAC <- c(BAC,mean(aBAC))
    BAC.avg <- c(BAC.avg,mean(aBAC.avg))
    BAC.majority <- c( BAC.majority,mean(aBAC.majority))  
  }
  
  result <- data.frame(AUC,AUC.avg,BAC,BAC.avg,BAC.majority)
  names(result) <- c("AUC.NOBagging","AUC.Bagging-avg","BAC.NoBagging","BAC.Bagging-avg","BAC.Bagging-majority")
  
  return (result)
  
}


bac <- function(targets, predictions ,th)
{
  
  #Calculating confusion matrix based on threshold
  prediction<- data.frame(targets,predictions)
  tp <- nrow(prediction[which(prediction$target == 1 & prediction$predictions>=th),])
  fn <- nrow(prediction[which(prediction$target == 1 & prediction$predictions <th ),])
  fp <- nrow(prediction[which(prediction$target == 0 & prediction$predictions >= th),])
  tn <- nrow(prediction[which(prediction$target == 0 & prediction$predictions <th),])
  
  #Calculating Bac
  class1<-ifelse((tp+fn==0),0,tp/(tp+fn))
  class0<-ifelse((tn+fp==0),0,tn/(tn+fp))
  BAC<-(1/2) *(class1 + class0 )
  
  #Print statistics
  cat("\nConfusion Matrix"," tp:", tp, " Fp:", fp," tn:", tn
      ," fn:", fn," sum:",(tp+fp+fn+tn))
  cat("\nfor positive case (majority) ",  tp/(tp+fn))
  cat("\nfor negative case (minority) ", tn/(tn+fp))
  
  
  return(BAC)
  
}


auc<-function(targets, predictions)
{
  
  
  AUC.pROC <- roc(targets,predictions)$auc
  cat( "  Auc pROC: ",AUC.pROC )
  return(AUC.pROC)
}
#==================================================Model=================
#      it gets train and test data as input to train the model and return the prediction result
#      Model output is supposed to be probabilities not the class label
nb<-function(trainingData,testingData)
{
  NB.model<-naiveBayes(trainingClass~.,trainingData)
  prediction <- round(predict( NB.model,testingData,type="raw")[,2],5)
  return(prediction)
}

##================================================formatting the data
dataFormat<-function(datafileDir)
{
  data.raw <- read.csv(datafileDir)
  trainingData <- data.raw[,-(1:(which(colnames(data.raw)=="ACTB")-1))]
  trainingClass <- rep(0,nrow(trainingData))
  for(i in 1:length(trainingClass))
  {
    if(data.raw$resp.simple[i]=="CR")
      trainingClass[i] =1
    else
      trainingClass[i]=0
  }
  data<-data.frame(trainingClass,trainingData)
  data$trainingClass<-as.factor(data$trainingClass)
  return(data)
}

#========================perform kfold cross validation==============================
#sample Usage
data<-dataFormat( "~/Dream9/trainingData-release.csv")
set.seed(42)
result<-kfoldCV(data,3,nb,5,5,40)
Path<-"~/Dream9/results/nb.csv"
write.csv(result, file=Path)
e<-evalMethod(result,5,0.5)
print(e)
summary(e)  
